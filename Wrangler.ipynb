{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#import netCDF4\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import pyproj\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from dispel4py.workflow_graph import WorkflowGraph \n",
    "from dispel4py.provenance import *\n",
    "\n",
    "from dispel4py.base import create_iterative_chain, ConsumerPE, IterativePE, SimpleFunctionPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadCSV(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('csv_output')\n",
    "        self._add_output('csv_file')\n",
    "    \n",
    "    def _process(self,inputs):\n",
    "        self.log('Reading CSV')\n",
    "        csvFile = inputs['input'][0]\n",
    "        csvMeta = inputs['csv_desc'][0]\n",
    "        \n",
    "        headerText = self.readHeaderLines(csvFile)\n",
    "        \n",
    "        self.invalidDateTime = np.datetime64('4000-01-01')\n",
    "        \n",
    "        # TODO: if needed port the json handling method\n",
    "        self.metaCSVdict = self.readJson(csvMeta)\n",
    "        delimiter = self.metaCSVdict['csvSeparator']\n",
    "        columnsList = []\n",
    "        columnsList.append(self.metaCSVdict['columnDate'])\n",
    "        columnsList.append(self.metaCSVdict['columnHour'])\n",
    "        columnsList.append(self.metaCSVdict['columnMinute'])\n",
    "        columnsList.append(self.metaCSVdict['columnX'])\n",
    "        columnsList.append(self.metaCSVdict['columnY'])        \n",
    "        \n",
    "        \n",
    "        self.dataUnsortedStr = np.recfromtxt(csvFile, skip_header=self.numHeaderLines, comments=\"#\", dtype=\"|S300\", delimiter=delimiter)\n",
    "        self.dataColumns = self.dataUnsortedStr[:, columnsList ]\n",
    "        \n",
    "        rowCounter = 0\n",
    "        queryDataArray = []        \n",
    "        for oneRow in self.dataColumns:\n",
    "            (utcTimeStr, utcTime) = self.decodeDateTime(dateStr=oneRow[0], hourStr=oneRow[1], minuteStr=oneRow[2])\n",
    "            if utcTimeStr==None:  # None means INVALID request!\n",
    "                dataRow = [rowCounter, self.invalidDateTime, \"INVALID\", float(oneRow[3]), float(oneRow[4])]\n",
    "                # store [id, utc-time, utc-time-str, X-coord, Y-coord ]\n",
    "                queryDataArray.append(dataRow)\n",
    "            else:\n",
    "                dataRow = [rowCounter, np.datetime64(utcTime), utcTimeStr, float(oneRow[3]), float(oneRow[4])]\n",
    "                # store [id, utc-time, utc-time-str, X-coord, Y-coord ]\n",
    "                queryDataArray.append(dataRow)\n",
    "            rowCounter += 1\n",
    "            \n",
    "        # Translate the python list to a 2 dimensional numpy array of [ [id, utc-time, X-coord, Y-coord], ... ]\n",
    "        queryDataNPA = np.array(queryDataArray)\n",
    "\n",
    "        self.timeUnits = \"\"  # self.metaData.variables['time'].units\n",
    "        self.dateTimeArray = queryDataNPA[:, 1]\n",
    "        \n",
    "        # remove invalid dateTime records from the array\n",
    "        indexDelete = np.where(self.dateTimeArray == self.invalidDateTime)  # reserved  for \"INVALID\"\n",
    "        self.dateTimeArrayClean = np.delete(self.dateTimeArray, indexDelete)\n",
    "        \n",
    "        # np.datetime64 => datetime; The date-time must be in UTC\n",
    "        self.minDateTime = np.min(self.dateTimeArrayClean).astype(datetime.datetime).replace(tzinfo=pytz.UTC)\n",
    "        self.maxDateTime = np.max(self.dateTimeArrayClean).astype(datetime.datetime).replace(tzinfo=pytz.UTC)\n",
    "\n",
    "        fmt = '%Y-%m-%d %H:%M:%S %Z'\n",
    "        self.minDateTime_str = self.minDateTime.strftime(fmt) \n",
    "        self.maxDateTime_str = self.maxDateTime.strftime(fmt)\n",
    "\n",
    "        queryDataNPAdt = queryDataNPA  # create sorted 2-dimensional array\n",
    "        self.queryDataNPAdt = queryDataNPAdt\n",
    "        \n",
    "        self.projFuncDefstring = self.metaCSVdict['projString']\n",
    "        self.projectionFunction = pyproj.Proj(self.projFuncDefstring)\n",
    "        \n",
    "        # 2-dimensional numpy array [  [id, utc-time, X-coord, Y-coord ], .. ] sorted by utc time\n",
    "        xcoords = queryDataNPAdt[:, 3]  # still a 1-dimensional numpy array of strings\n",
    "        ycoords = queryDataNPAdt[:, 4]  # still a 1-dimensional numpy array of strings\n",
    "        \n",
    "        (longitudes,latitudes) = self.unproject2LongitudeLatitudes(xcoords, ycoords)\n",
    "        lonLatStacked = np.vstack((longitudes,latitudes)).T\n",
    "        # print lonLatStacked\n",
    "\n",
    "        # Determine the bounding box.\n",
    "        self.llbox_west = np.min(longitudes)\n",
    "        self.llbox_east = np.max(longitudes)\n",
    "        self.llbox_north = np.max(latitudes)\n",
    "        self.llbox_south = np.min(latitudes)\n",
    "    \n",
    "        self.queryDataNPAdtsLL = np.vstack((queryDataNPAdt[:, 0], queryDataNPAdt[:, 1], longitudes, latitudes)).T\n",
    "        \n",
    "        # self.log(self.queryDataNPAdtsLL)\n",
    "\n",
    "        self.write('csv_file', csvFile)\n",
    "        self.write('csv_output', self.queryDataNPAdtsLL)\n",
    "        \n",
    "    \n",
    "    def readHeaderLines(self, fileName):\n",
    "        self.numHeaderLines = 1\n",
    "        headerText = \"\"\n",
    "        n = self.numHeaderLines\n",
    "        ftxt = open(fileName,'rU')\n",
    "        while n>0:\n",
    "            ln = ftxt.readline()\n",
    "            if not ln:\n",
    "                break\n",
    "            headerText += ln.rstrip('\\n')  \n",
    "            n -= 1\n",
    "        ftxt.close()\n",
    "        return headerText\n",
    "    \n",
    "    def readJson(self, fileName):\n",
    "        with open(fileName) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "    def decodeDateTime(self, dateStr, hourStr, minuteStr):\n",
    "\n",
    "        if self.metaCSVdict['dateFormat'] == \"%d%b%y\":\n",
    "            givenDate = datetime.datetime.strptime(dateStr, \"%d%b%y\")\n",
    "            \n",
    "        if self.metaCSVdict['hourFormat'] == \"hourInterval\":\n",
    "            try:\n",
    "                if '-' in hourStr:\n",
    "                    hour = float(hourStr.split('-')[0])\n",
    "                else:\n",
    "                    hour = float(hourStr[:3])\n",
    "            except:\n",
    "                if self.autoResolve_hour>0:\n",
    "                    hour = self.autoResolve_hour\n",
    "                else:\n",
    "                    try:\n",
    "                        minute = int(minuteStr)\n",
    "                        self.log('WARNING: could not extract (hour) from: \"(%s)\"' %(hourStr) )\n",
    "                    except:\n",
    "                        self.log('WARNING: could not extract (hour & minute) from: \"(%s,%s)\"' %(hourStr, minuteStr))\n",
    "                    \n",
    "                    return None,None  # this mean INVALID request\n",
    "\n",
    "            try:\n",
    "                minute = int(minuteStr)\n",
    "            except:\n",
    "                if self.autoResolve_minute>0:\n",
    "                    minute = self.autoResolve_minute\n",
    "                else:\n",
    "                    self.log('WARNING: could not extract minute from: \"%s\"' %(minuteStr) )\n",
    "                    return None, None  # this mean INVALID request\n",
    "\n",
    "            \n",
    "        localTime = givenDate + timedelta(hours=hour, minutes=minute)\n",
    "        \n",
    "        (utcTime, datetime_in_utc_str, datetime_with_tz_str) = self.convertLocalDateTime2Utc(localTime, zone=self.metaCSVdict['timeZone'])\n",
    "        \n",
    "        fmt = '%Y-%m-%d %H:%M:%S %Z'\n",
    "        utcTimeStr = utcTime.strftime(fmt)\n",
    "        return (utcTimeStr, utcTime)\n",
    "        \n",
    "    def convertLocalDateTime2Utc(self,datetime_without_tz, zone):\n",
    "        fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\n",
    "       \n",
    "        local_tz = pytz.timezone(zone)\n",
    "\n",
    "        needToDecide = False\n",
    "        try:\n",
    "            datetime_with_tz = local_tz.localize(datetime_without_tz, is_dst=None)  # No daylight saving time\n",
    "        except pytz.exceptions.AmbiguousTimeError:\n",
    "            needToDecide = True\n",
    "        except pytz.exceptions.NonExistentTimeError:\n",
    "            needToDecide = True\n",
    "        if needToDecide:\n",
    "            datetime_with_tz = local_tz.localize(datetime_without_tz, is_dst=False)\n",
    "        \n",
    "        datetime_in_utc = datetime_with_tz.astimezone(pytz.utc)\n",
    "\n",
    "        datetime_with_tz_str = datetime_with_tz.strftime(fmt)\n",
    "        datetime_in_utc_str = datetime_in_utc.strftime(fmt)\n",
    "\n",
    "        return (datetime_in_utc, datetime_in_utc_str, datetime_with_tz_str)\n",
    "    \n",
    "    def unproject2LongitudeLatitudes(self, xcoords, ycoords):\n",
    "        LL  = self.projectionFunction(xcoords, ycoords, inverse=True)\n",
    "        longitudes = LL[0]\n",
    "        latitudes = LL[1]\n",
    "        return (longitudes, latitudes)  # tuple, vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadNETCDF(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('netcdf_out')\n",
    "    \n",
    "    def _process(self,inputs):\n",
    "        self.log('Reading NetCDF')\n",
    "        netcdfFile = inputs['input'][0]\n",
    "        fmt = '%Y-%m-%d %H:%M:%S %Z'\n",
    "        \n",
    "        #open NetCDF dataset with xarray\n",
    "        ds = xr.open_dataset(netcdfFile)\n",
    "        \n",
    "        #Time variables\n",
    "        dt = ds['time'] #date-time DataArray\n",
    "        timeDim = dt.shape[0]\n",
    "         \n",
    "        minDateTimeArray = np.min(dt) #returns minDate in a DataArray\n",
    "        maxDateTimeArray = np.max(dt)\n",
    "        minDateTime = minDateTimeArray.values #gets the value out of the DataArray\n",
    "        maxDateTime = maxDateTimeArray.values\n",
    "        \n",
    "        minDateTime_conv = pd.to_datetime(minDateTime) #converts numpy datetime64 to datetime (better for readability)\n",
    "        # minDataTime_str = minDateTime_conv.strftime(fmt)\n",
    "        maxDateTime_conv = pd.to_datetime(maxDateTime)\n",
    "        # maxDataTime_str = maxDateTime_conv.strftime(fmt)\n",
    "        \n",
    "        \n",
    "        timeDelta = np.timedelta64(dt[1].values - dt[0].values, 'm') #time delta between consecutive measurements in minutes\n",
    "        \n",
    "        #Spatial variables\n",
    "        dx = ds['x']\n",
    "        dy = ds['y']\n",
    "        xDim = dx.shape[0]\n",
    "        yDim = dy.shape[0]\n",
    "        gridSize = xDim * yDim\n",
    "        \n",
    "        xAxis = dx.data[:]\n",
    "        yAxis = dy.data[:]\n",
    "        \n",
    "        self.xcoords, self.ycoords  = np.meshgrid(xAxis, yAxis)\n",
    "        \n",
    "        self.projFuncDefstring = ds['projection'].proj4_params\n",
    "        self.projectionFunction = pyproj.Proj(self.projFuncDefstring)\n",
    "        \n",
    "        (self.longitudes,self.latitudes) = self.unproject2LongitudeLatitudes(self.xcoords, self.ycoords)\n",
    "        self.lonLatStacked = np.hstack((self.longitudes.reshape(gridSize,1),self.latitudes.reshape(gridSize,1)))\n",
    "        \n",
    "        self.llbox_west = np.min(self.longitudes)\n",
    "        self.llbox_east = np.max(self.longitudes)\n",
    "        self.llbox_north = np.max(self.latitudes)\n",
    "        self.llbox_south = np.min(self.latitudes)\n",
    "        \n",
    "        ds.__setitem__('lonLatStacked', (['locations', 'coordinates'], self.lonLatStacked))\n",
    "\n",
    "        self.write('netcdf_out', ds)\n",
    "        \n",
    "\n",
    "        \n",
    "    def unproject2LongitudeLatitudes(self, xcoords, ycoords):\n",
    "        LL = self.projectionFunction(xcoords, ycoords, inverse=True)\n",
    "        longitudes = LL[0]\n",
    "        latitudes = LL[1]\n",
    "        return (longitudes, latitudes) # tuple, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('csvData', grouping=[1])\n",
    "        self._add_input('netcdfData', grouping=[1])\n",
    "        self._add_output('output')\n",
    "        self.csv=[]\n",
    "        self.netcdf=[]\n",
    " \n",
    "    \n",
    "    def _process(self,inputs):\n",
    "        self.log('Processing Data')\n",
    "        \n",
    "        #function used for getting the distance between 2 points\n",
    "        geoTransfWGS84 = pyproj.Geod(ellps='WGS84')\n",
    "        self.geoTransf = geoTransfWGS84\n",
    "        \n",
    "        if 'csvData' in inputs.keys():\n",
    "            self.csv.append(inputs['csvData'])\n",
    "        \n",
    "        if 'netcdfData' in inputs.keys():\n",
    "            self.netcdf.append(inputs['netcdfData'])\n",
    "        \n",
    "        if (len(self.csv)>0 and len(self.netcdf)>0):\n",
    "            self.csvData = self.csv.pop(0)\n",
    "            self.netcdfData = self.netcdf.pop(0)\n",
    "            \n",
    "            self.dateTimeArray = self.netcdfData['time'].values\n",
    "            \n",
    "            dx = self.netcdfData['x']\n",
    "            self.xDim = dx.shape[0]\n",
    "\n",
    "            variableName = 'precipitation_amount'\n",
    "            valueList = []\n",
    "\n",
    "            for timeLonLat in self.csvData:\n",
    "                idn = timeLonLat[0]\n",
    "                utcTime = timeLonLat[1]\n",
    "                lon = timeLonLat[2]\n",
    "                lat = timeLonLat[3]\n",
    "                value = self.GetValue_time_lon_lat(variableName, utcTime, lon, lat)\n",
    "                valueList.append(value)\n",
    "            valueArray = np.array(valueList)\n",
    "\n",
    "            processedData = np.hstack((self.csvData, valueArray.reshape(valueArray.shape[0], 1)))\n",
    "\n",
    "            self.write('output', processedData)\n",
    "        \n",
    "\n",
    "        \n",
    "    def GetValue_time_lon_lat(self, variableName, utcTime, lon, lat):\n",
    "        \n",
    "        closestDateTimeIndex = self.FindClosestDateTimeIndex(utcTime)\n",
    "        minDistanceDataIndex = self.FindClosestLonLatPointIndex(lon, lat)\n",
    "        dataValue = self.GetDataAtIndex(closestDateTimeIndex, minDistanceDataIndex, variableName=variableName)\n",
    "\n",
    "        return dataValue\n",
    "    \n",
    "    \n",
    "    def FindClosestDateTimeIndex(self, time):\n",
    "        closestTime = self.FindClosestDateTime(time, self.dateTimeArray)\n",
    "        closestTimeIndex = np.where(self.dateTimeArray == closestTime)[0][0]\n",
    "        return closestTimeIndex\n",
    "    \n",
    "    def FindClosestDateTime(self, givenDateTime, dateTimeList):\n",
    "        pivot = np.datetime64(givenDateTime)\n",
    "        result = min(dateTimeList, key=lambda x: abs(x - pivot))\n",
    "        return result\n",
    "    \n",
    "    def FindClosestLonLatPointIndex(self, lon, lat):\n",
    "        self.givenLon = lon\n",
    "        self.givenLat = lat\n",
    "    \n",
    "        idx = 0\n",
    "        distArray = np.zeros(self.netcdfData['lonLatStacked'].shape[0])\n",
    "        coordinatesArray = self.netcdfData['lonLatStacked'].values\n",
    "        for tupleLL in coordinatesArray:\n",
    "            dist = self.Distance2pointsInLonLat(lon, lat, tupleLL[0], tupleLL[1])\n",
    "            distArray[idx] = dist\n",
    "            idx +=1\n",
    "        minDist = np.min(distArray)\n",
    "        minDistIndex = np.where(distArray == minDist)[0][0]\n",
    "        return minDistIndex\n",
    "    \n",
    "    def Distance2pointsInLonLat(self, lng1,lat1,lng2,lat2):\n",
    "        #global geoTransfWGS84\n",
    "        #geoTransfWGS84\n",
    "        az12,az21,dist = self.geoTransf.inv(lng1,lat1,lng2,lat2)\n",
    "        return dist\n",
    "        \n",
    "    def GetDataAtIndex(self, timeIndex, dataIndex, variableName='precipitation_amount'):\n",
    "        idX = dataIndex % self.xDim\n",
    "        idY = dataIndex / self.xDim\n",
    "\n",
    "        # variableFound = self.GetVariable(variableName)\n",
    "        # if variableFound:\n",
    "        #     dataValue = variableFound[timeIndex][idY][idX]\n",
    "        #     return dataValue\n",
    "        # else:\n",
    "        #     return None\n",
    "\n",
    "        # 'image1_image_data' holds the 'precipitation_amount' data\n",
    "        valueDataArray = self.netcdfData['image1_image_data'][timeIndex][idY][idX]\n",
    "        dataValue = valueDataArray.values.item(0)\n",
    "        return dataValue\n",
    "    \n",
    "    \n",
    "    #needs to be adapted to xarray\n",
    "    def GetVariable(self, variableName):\n",
    "        keylist = self.metaData.variables.keys()\n",
    "        variableFound = None\n",
    "        for k in keylist:\n",
    "            try:\n",
    "                if self.metaData.variables[k].standard_name == variableName:\n",
    "                    #  self.metaData.variables['image1_image_data'].standard_name == variableName ..\n",
    "                    variableFound = self.metaData.variables[k]\n",
    "            except:\n",
    "                pass\n",
    "        return variableFound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreData(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('wrangler_output')\n",
    "        self._add_input('csv_file')\n",
    "        self.csv = False\n",
    "        self.wrangler = False\n",
    "         \n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Storing Data')\n",
    "        outputLocation = \"output/wrangled_data.csv\"\n",
    "        parameterList = [\"utc-time\", \"longitude\", \"latitude\", \"precipitation_amount\"]\n",
    "\n",
    "        if 'csv_file' in inputs.keys():\n",
    "            csvFile = inputs['csv_file']\n",
    "            self.csvHeader = self.readHeaderLines(csvFile)\n",
    "            self.csvContent = np.recfromtxt(csvFile, skip_header=1, comments=\"#\", dtype=\"|S300\", delimiter=',')\n",
    "            self.csv = True\n",
    "\n",
    "        if 'wrangler_output' in inputs.keys():\n",
    "            self.wranglerResults = inputs['wrangler_output']\n",
    "            self.wrangler = True\n",
    "\n",
    "        if self.csv and self.wrangler:\n",
    "\n",
    "            for item in parameterList:\n",
    "                self.csvHeader += ',%s' %item\n",
    "\n",
    "            finalStack = np.hstack((self.csvContent, self.wranglerResults[:,1:]))\n",
    "\n",
    "            np.savetxt(outputLocation, finalStack, fmt='%s', delimiter=',', header=self.csvHeader)\n",
    "\n",
    "\n",
    "    def readHeaderLines(self, fileName):\n",
    "        self.numHeaderLines = 1\n",
    "        headerText = \"\"\n",
    "        n = self.numHeaderLines\n",
    "        ftxt = open(fileName, 'rU')\n",
    "        while n > 0:\n",
    "            ln = ftxt.readline()\n",
    "            if not ln:\n",
    "                break\n",
    "            headerText += ln.rstrip('\\n')\n",
    "            n -= 1\n",
    "        ftxt.close()\n",
    "        return headerText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWorkflow():\n",
    "    read_csv = ReadCSV()\n",
    "    read_csv.name = 'CSV'\n",
    "    read_netcdf = ReadNETCDF()\n",
    "    read_netcdf.name = 'NETCDF'\n",
    "    compute = ProcessData()\n",
    "    store = StoreData()\n",
    "    store.name = 'STORE'\n",
    "    \n",
    "    graph = WorkflowGraph()\n",
    "    graph.connect(read_csv, 'csv_output', compute, 'csvData')\n",
    "    graph.connect(read_netcdf, 'netcdf_out', compute, 'netcdfData')\n",
    "    graph.connect(read_csv, 'csv_file', store, 'csv_file')\n",
    "    graph.connect(compute, 'output', store, 'wrangler_output')\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "workflow_graph = createWorkflow()\n",
    "\n",
    "from dispel4py.visualisation import display\n",
    "display(workflow_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {     \n",
    "                'CSV': [{ 'input': [ 'data/accident_data.csv'], 'csv_desc': ['data/metaDataCsv.json'] }],\n",
    "                'NETCDF': [{ 'input': ['http://opendap.knmi.nl/knmi/thredds/dodsC/DATALAB/hackathon/radarFull2006.nc'] }]\n",
    "                #'NETCDF': [ { 'input': [ 'data/radarFull2006.nc'] }] too large for GitHub\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWorkflow():\n",
    "                                                     \n",
    "    print input_data                   \n",
    "\n",
    "    #Launch in simple process\n",
    "    result = simple_process.process_and_return(workflow_graph, input_data)\n",
    "    print \"\\n RESULT: \"+str(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runWorkflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
